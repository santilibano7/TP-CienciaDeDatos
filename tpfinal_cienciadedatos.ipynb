 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ TP Final - Ciencia de Datos\n",
    "## Entrenamiento de Modelo de Lenguaje para Generaci√≥n de Rese√±as\n",
    "\n",
    "Este notebook entrena un modelo de lenguaje para generar rese√±as de productos de Amazon.\n",
    "\n",
    "### üìã Objetivos:\n",
    "- Cargar y preparar datos de rese√±as de Amazon\n",
    "- Entrenar un modelo de lenguaje para generar rese√±as\n",
    "- Evaluar la calidad de las rese√±as generadas\n",
    "- Analizar la coherencia entre ratings y sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling, pipeline\n",
    "import torch\n",
    "import re\n",
    "\n",
    "# Para traducci√≥n\n",
    "try:\n",
    "    from googletrans import Translator\n",
    "    GOOGLE_TRANS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GOOGLE_TRANS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è googletrans no disponible. Instalar con: pip install googletrans==4.0.0rc1\")\n",
    "\n",
    "try:\n",
    "    from transformers import MarianMTModel, MarianTokenizer\n",
    "    MARIAN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MARIAN_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è MarianMT no disponible. Instalar con: pip install sentencepiece\")\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Verificaci√≥n de Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar GPU\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"Usando CPU - optimizando para mejor rendimiento\")\n",
    "    # Optimizaciones para CPU\n",
    "    torch.set_num_threads(8)  # Usar m√°s n√∫cleos de CPU\n",
    "    print(f\"N√∫cleos CPU utilizados: {torch.get_num_threads()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Carga y Preparaci√≥n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer dataset\n",
    "print(\"üìñ Cargando dataset...\")\n",
    "df = pd.read_csv('Amazon_Unlocked_Mobile.csv')\n",
    "df = df.dropna(subset=['Reviews', 'Rating'])  # eliminamos rese√±as y ratings vac√≠os\n",
    "df = df[['Product Name', 'Brand Name', 'Price', 'Rating', 'Reviews']]\n",
    "\n",
    "# Convertir rating a entero\n",
    "df['Rating'] = df['Rating'].astype(int)\n",
    "\n",
    "print(f\"üìä Dataset cargado: {len(df)} registros\")\n",
    "print(f\"üìà Distribuci√≥n de ratings:\")\n",
    "print(df['Rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancear el dataset: igual cantidad de rese√±as por rating\n",
    "max_per_rating = 800  # Aumentado para mejor aprendizaje\n",
    "balanced_df = pd.concat([\n",
    "    df[df['Rating'] == rating].sample(n=min(max_per_rating, len(df[df['Rating'] == rating])), random_state=42)\n",
    "    for rating in range(1, 6)\n",
    "])\n",
    "\n",
    "print(f\"‚öñÔ∏è Dataset balanceado: {len(balanced_df)} registros\")\n",
    "print(f\"üìä Distribuci√≥n por rating:\")\n",
    "for rating in range(1, 6):\n",
    "    count = len(balanced_df[balanced_df['Rating'] == rating])\n",
    "    print(f\"  {rating} estrellas: {count} rese√±as\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Funciones de Limpieza y Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(texto):\n",
    "    \"\"\"Limpia el texto de caracteres especiales y normaliza espacios\"\"\"\n",
    "    if pd.isna(texto):\n",
    "        return \"\"\n",
    "    \n",
    "    texto = str(texto)\n",
    "    # Remover caracteres especiales pero mantener puntuaci√≥n b√°sica\n",
    "    texto = re.sub(r'[^\\w\\s\\.\\,\\!\\?\\-\\']', '', texto)\n",
    "    # Normalizar espacios\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def crear_prompt_mejorado(row):\n",
    "    \"\"\"Crea un prompt m√°s estructurado y espec√≠fico\"\"\"\n",
    "    rating = row['Rating']\n",
    "    producto = limpiar_texto(row['Product Name'] or 'Producto')\n",
    "    marca = limpiar_texto(row['Brand Name'] or 'Marca')\n",
    "    precio = str(row['Price'])\n",
    "    rese√±a = limpiar_texto(row['Reviews'])\n",
    "    \n",
    "    # Determinar sentimiento esperado basado en rating\n",
    "    if rating == 1:\n",
    "        sentimiento = \"muy negativo\"\n",
    "        instruccion = \"Escribe una rese√±a muy cr√≠tica y negativa\"\n",
    "    elif rating == 2:\n",
    "        sentimiento = \"negativo\"\n",
    "        instruccion = \"Escribe una rese√±a negativa con algunos aspectos positivos\"\n",
    "    elif rating == 3:\n",
    "        sentimiento = \"neutral\"\n",
    "        instruccion = \"Escribe una rese√±a equilibrada con pros y contras\"\n",
    "    elif rating == 4:\n",
    "        sentimiento = \"positivo\"\n",
    "        instruccion = \"Escribe una rese√±a positiva con algunas cr√≠ticas menores\"\n",
    "    else:  # rating == 5\n",
    "        sentimiento = \"muy positivo\"\n",
    "        instruccion = \"Escribe una rese√±a muy positiva y entusiasta\"\n",
    "    \n",
    "    # Prompt mejorado con instrucciones claras\n",
    "    prompt = f\"\"\"INSTRUCCI√ìN: {instruccion} para este producto.\n",
    "\n",
    "PRODUCTO: {producto}\n",
    "MARCA: {marca}\n",
    "PRECIO: {precio}\n",
    "RATING: {rating} estrellas ({sentimiento})\n",
    "\n",
    "RESE√ëA: {rese√±a}\n",
    "\n",
    "FIN\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "print(\"‚úÖ Funciones de procesamiento definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Creaci√≥n de Prompts Estructurados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el nuevo prompt\n",
    "balanced_df['text'] = balanced_df.apply(crear_prompt_mejorado, axis=1)\n",
    "\n",
    "print('üìù Ejemplo de texto mejorado:')\n",
    "print(balanced_df['text'].iloc[0])\n",
    "print('Tipo:', type(balanced_df['text'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset de HuggingFace\n",
    "dataset = Dataset.from_pandas(balanced_df[['text']])\n",
    "\n",
    "# Dividir dataset en entrenamiento y validaci√≥n\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"üìö Dataset de entrenamiento: {len(train_dataset)} ejemplos\")\n",
    "print(f\"üîç Dataset de validaci√≥n: {len(eval_dataset)} ejemplos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî† Tokenizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar tokenizador con modelo m√°s grande\n",
    "model_name = \"gpt2\"  # Cambiado a gpt2 completo para mejor capacidad\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Aumentamos la longitud m√°xima a 256 tokens para capturar mejor el contexto\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,  # Aumentado significativamente\n",
    "    )\n",
    "\n",
    "print('üî§ Tokenizador configurado')\n",
    "print('üìù Ejemplo de tokenizaci√≥n:')\n",
    "print(tokenize_function({\"text\": [balanced_df['text'].iloc[0]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar datasets\n",
    "print(\"üîÑ Tokenizando datasets...\")\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train = tokenized_train.remove_columns(['text'])\n",
    "\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = tokenized_eval.remove_columns(['text'])\n",
    "\n",
    "print(\"‚úÖ Tokenizaci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Configuraci√≥n del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "print(f\"ü§ñ Modelo {model_name} cargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar argumentos de entrenamiento optimizados\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,  # Reducido para evitar overfitting\n",
    "    per_device_train_batch_size=4,  # Reducido para el modelo m√°s grande\n",
    "    save_steps=200,  # Guardar m√°s frecuentemente\n",
    "    save_total_limit=3,  # Mantener 3 checkpoints\n",
    "    logging_steps=50,  # Logging m√°s frecuente para monitoreo\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    # Optimizaciones para aprendizaje √≥ptimo\n",
    "    dataloader_num_workers=2,  # Workers moderados para Windows\n",
    "    gradient_accumulation_steps=8,  # Acumular gradientes para batch efectivo de 32\n",
    "    warmup_steps=100,  # Warmup apropiado\n",
    "    learning_rate=1e-4,  # Learning rate m√°s alto para mejor aprendizaje\n",
    "    weight_decay=0.01,  # Regularizaci√≥n\n",
    "    # Optimizaciones adicionales\n",
    "    fp16=False,  # Desactivar para CPU\n",
    "    bf16=False,  # Desactivar para CPU\n",
    "    optim=\"adamw_torch\",  # Optimizador eficiente\n",
    "    lr_scheduler_type=\"cosine\",  # Scheduler √≥ptimo\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    evaluation_strategy=\"steps\",  # Evaluar durante el entrenamiento\n",
    "    eval_steps=200,  # Evaluar cada 200 pasos\n",
    ")\n",
    "\n",
    "print(\"‚öôÔ∏è Configuraci√≥n de entrenamiento optimizada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar data collator y trainer\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"üéØ Trainer configurado y listo para entrenar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar entrenamiento\n",
    "print(\"üöÄ Iniciando entrenamiento...\")\n",
    "print(\"‚è±Ô∏è Esto puede tomar varios minutos...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"‚úÖ Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Guardado del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo y tokenizer\n",
    "print(\"üíæ Guardando modelo y tokenizer...\")\n",
    "\n",
    "model.save_pretrained('./results')\n",
    "tokenizer.save_pretrained('./results')\n",
    "\n",
    "print(\"‚úÖ Modelo guardado en './results'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≠ Funciones de Generaci√≥n Mejoradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_prompt_generacion(producto, marca, precio, rating):\n",
    "    \"\"\"Crea un prompt espec√≠fico para generaci√≥n\"\"\"\n",
    "    if rating == 1:\n",
    "        sentimiento = \"muy negativo\"\n",
    "        instruccion = \"Escribe una rese√±a muy cr√≠tica y negativa\"\n",
    "    elif rating == 2:\n",
    "        sentimiento = \"negativo\"\n",
    "        instruccion = \"Escribe una rese√±a negativa con algunos aspectos positivos\"\n",
    "    elif rating == 3:\n",
    "        sentimiento = \"neutral\"\n",
    "        instruccion = \"Escribe una rese√±a equilibrada con pros y contras\"\n",
    "    elif rating == 4:\n",
    "        sentimiento = \"positivo\"\n",
    "        instruccion = \"Escribe una rese√±a positiva con algunas cr√≠ticas menores\"\n",
    "    else:  # rating == 5\n",
    "        sentimiento = \"muy positivo\"\n",
    "        instruccion = \"Escribe una rese√±a muy positiva y entusiasta\"\n",
    "    \n",
    "    prompt = f\"\"\"INSTRUCCI√ìN: {instruccion} para este producto.\n",
    "\n",
    "PRODUCTO: {producto}\n",
    "MARCA: {marca}\n",
    "PRECIO: {precio}\n",
    "RATING: {rating} estrellas ({sentimiento})\n",
    "\n",
    "RESE√ëA:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def generar_resena_mejorada(producto, marca, precio, rating):\n",
    "    \"\"\"Genera una rese√±a mejorada con mejor control\"\"\"\n",
    "\n",
    "    prompt = crear_prompt_generacion(producto, marca, precio, rating)\n",
    "\n",
    "    # Configurar el generador con par√°metros optimizados\n",
    "    generator = pipeline(\n",
    "        'text-generation',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=80,  # Generar hasta 80 tokens nuevos\n",
    "        do_sample=True,\n",
    "        temperature=0.7,  # Temperatura moderada para balance entre creatividad y coherencia\n",
    "        top_p=0.85,  # Nucleus sampling\n",
    "        top_k=40,  # Top-k sampling\n",
    "        repetition_penalty=1.3,  # Penalizar repeticiones\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        # Configuraci√≥n adicional\n",
    "        num_beams=1,  # Sin beam search para m√°s creatividad\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=3,  # Evitar repetici√≥n de n-gramas\n",
    "    )\n",
    "\n",
    "    # Generar texto\n",
    "    resultado = generator(prompt, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "    # Extraer solo la rese√±a generada\n",
    "    if \"RESE√ëA:\" in resultado:\n",
    "        rese√±a = resultado.split(\"RESE√ëA:\")[1].split(\"FIN\")[0].strip()\n",
    "    else:\n",
    "        rese√±a = resultado[len(prompt):].strip()\n",
    "\n",
    "    # Limpiar la rese√±a\n",
    "    rese√±a = re.sub(r'\\s+', ' ', rese√±a).strip()\n",
    "    \n",
    "    # Si la rese√±a es muy corta, intentar generar m√°s\n",
    "    if len(rese√±a.split()) < 10:\n",
    "        # Continuar la generaci√≥n\n",
    "        prompt_continuacion = resultado + \" \"\n",
    "        resultado2 = generator(prompt_continuacion, num_return_sequences=1)[0]['generated_text']\n",
    "        parte2 = resultado2[len(prompt_continuacion):].strip()\n",
    "        rese√±a = rese√±a + \" \" + parte2\n",
    "        rese√±a = re.sub(r'\\s+', ' ', rese√±a).strip()\n",
    "\n",
    "    return rese√±a\n",
    "\n",
    "def analizar_sentimiento(rese√±a):\n",
    "    \"\"\"Analiza el sentimiento de una rese√±a\"\"\"\n",
    "    rese√±a_lower = rese√±a.lower()\n",
    "    \n",
    "    # Palabras positivas y negativas m√°s espec√≠ficas\n",
    "    palabras_positivas = [\n",
    "        'good', 'great', 'excellent', 'love', 'like', 'amazing', 'perfect', 'wonderful',\n",
    "        'fantastic', 'awesome', 'outstanding', 'superb', 'brilliant', 'fabulous',\n",
    "        'satisfied', 'happy', 'pleased', 'impressed', 'recommend', 'best', 'quality'\n",
    "    ]\n",
    "    \n",
    "    palabras_negativas = [\n",
    "        'bad', 'terrible', 'awful', 'hate', 'dislike', 'horrible', 'worst', 'disappointed',\n",
    "        'poor', 'cheap', 'broken', 'defective', 'useless', 'waste', 'regret', 'avoid',\n",
    "        'problem', 'issue', 'faulty', 'unreliable', 'slow', 'expensive', 'overpriced'\n",
    "    ]\n",
    "    \n",
    "    positivas = sum(1 for palabra in palabras_positivas if palabra in rese√±a_lower)\n",
    "    negativas = sum(1 for palabra in palabras_negativas if palabra in rese√±a_lower)\n",
    "    \n",
    "    return positivas, negativas\n",
    "\n",
    "print(\"‚úÖ Funciones de generaci√≥n definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Prueba de Generaci√≥n de Rese√±as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar generaciones mejoradas\n",
    "print(\"üéØ Generando rese√±as de ejemplo con el modelo mejorado...\\n\")\n",
    "\n",
    "# Ejemplos de productos\n",
    "productos = [\n",
    "    (\"iPhone 15 Pro\", \"Apple\", \"$999\", 1),\n",
    "    (\"Samsung Galaxy S24\", \"Samsung\", \"$799\", 2),\n",
    "    (\"Google Pixel 8\", \"Google\", \"$699\", 3),\n",
    "    (\"OnePlus 12\", \"OnePlus\", \"$599\", 4),\n",
    "    (\"Xiaomi 14\", \"Xiaomi\", \"$499\", 5)\n",
    "]\n",
    "\n",
    "for producto, marca, precio, rating in productos:\n",
    "    print(f\"üì± {producto} ({marca}) - {precio} - {rating}‚≠ê\")\n",
    "    rese√±a = generar_resena_mejorada(producto, marca, precio, rating)\n",
    "    print(f\"üìù Rese√±a: {rese√±a}\")\n",
    "    print(f\"üìä Longitud: {len(rese√±a.split())} palabras\")\n",
    "\n",
    "    # Analizar sentimiento\n",
    "    positivas, negativas = analizar_sentimiento(rese√±a)\n",
    "    print(f\"üòä Palabras positivas: {positivas}\")\n",
    "    print(f\"üòû Palabras negativas: {negativas}\")\n",
    "\n",
    "    # Verificar si el sentimiento coincide con el rating\n",
    "    if rating >= 4 and positivas > negativas:\n",
    "        print(\"‚úÖ Sentimiento COINCIDE con rating alto\")\n",
    "    elif rating <= 2 and negativas > positivas:\n",
    "        print(\"‚úÖ Sentimiento COINCIDE con rating bajo\")\n",
    "    elif rating == 3 and abs(positivas - negativas) <= 2:\n",
    "        print(\"‚úÖ Sentimiento COINCIDE con rating neutral\")\n",
    "    else:\n",
    "        print(\"‚ùå Sentimiento NO coincide con rating\")\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar m√©tricas de entrenamiento de forma segura\n",
    "print(\"M√©tricas de entrenamiento:\")\n",
    "\n",
    "if hasattr(trainer, 'state'):\n",
    "    print(f\"Pasos totales: {trainer.state.global_step}\")\n",
    "\n",
    "    # Mostrar todas las m√©tricas disponibles\n",
    "    if trainer.state.log_history:\n",
    "        print(\"üìä Historial de m√©tricas:\")\n",
    "        for i, log in enumerate(trainer.state.log_history[-5:]):  # √öltimos 5 logs\n",
    "            print(f\"  Paso {log.get('step', 'N/A')}:\")\n",
    "            for key, value in log.items():\n",
    "                if key != 'step':\n",
    "                    print(f\"    {key}: {value}\")\n",
    "    else:\n",
    "        print(\"No hay historial de m√©tricas disponible\")\n",
    "else:\n",
    "    print(\"No se encontr√≥ informaci√≥n del estado del trainer\")\n",
    "\n",
    "print(\"\\nüéâ ¬°Entrenamiento completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Funciones Adicionales de Traducci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traducir_con_google(texto, idioma_destino='es'):\n",
    "    \"\"\"Traduce usando Google Translate\"\"\"\n",
    "    if not GOOGLE_TRANS_AVAILABLE:\n",
    "        return \"Traducci√≥n no disponible - instalar googletrans\"\n",
    "\n",
    "    try:\n",
    "        translator = Translator()\n",
    "        traduccion = translator.translate(texto, dest=idioma_destino)\n",
    "        return traduccion.text\n",
    "    except Exception as e:\n",
    "        return f\"Error en traducci√≥n: {e}\"\n",
    "\n",
    "def traducir_con_marian(texto, idioma_origen='en', idioma_destino='es'):\n",
    "    \"\"\"Traduce usando MarianMT (m√°s preciso)\"\"\"\n",
    "    if not MARIAN_AVAILABLE:\n",
    "        return \"Traducci√≥n no disponible - instalar sentencepiece\"\n",
    "\n",
    "    try:\n",
    "        # Modelo espec√≠fico para ingl√©s a espa√±ol\n",
    "        model_name = f'Helsinki-NLP/opus-mt-{idioma_origen}-{idioma_destino}'\n",
    "        tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "        model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "        # Tokenizar y traducir\n",
    "        inputs = tokenizer(texto, return_tensors=\"pt\", padding=True)\n",
    "        translated = model.generate(**inputs)\n",
    "        traduccion = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "        return traduccion\n",
    "    except Exception as e:\n",
    "        return f\"Error en traducci√≥n: {e}\"\n",
    "\n",
    "def traducir_resena(resena, metodo='marian', idioma_destino='es'):\n",
    "    \"\"\"Traduce una rese√±a usando el m√©todo especificado\"\"\"\n",
    "\n",
    "    if metodo == 'google':\n",
    "        return traducir_con_google(resena, idioma_destino)\n",
    "    elif metodo == 'marian':\n",
    "        return traducir_con_marian(resena, 'en', idioma_destino)\n",
    "    else:\n",
    "        return \"M√©todo de traducci√≥n no v√°lido\"\n",
    "\n",
    "print(\"‚úÖ Funciones de traducci√≥n definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåç Prueba de Traducci√≥n (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de traducci√≥n de una rese√±a generada\n",
    "print(\"üåç Probando traducci√≥n de rese√±as...\\n\")\n",
    "\n",
    "# Generar una rese√±a de ejemplo\n",
    "rese√±a_ejemplo = generar_resena_mejorada(\"iPhone 15 Pro\", \"Apple\", \"$999\", 5)\n",
    "print(f\"üìù Rese√±a original: {rese√±a_ejemplo}\")\n",
    "\n",
    "# Traducir usando MarianMT\n",
    "if MARIAN_AVAILABLE:\n",
    "    traduccion = traducir_resena(rese√±a_ejemplo, metodo='marian')\n",
    "    print(f\"üåç Traducci√≥n (MarianMT): {traduccion}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è MarianMT no disponible para traducci√≥n\")\n",
    "\n",
    "# Traducir usando Google Translate\n",
    "if GOOGLE_TRANS_AVAILABLE:\n",
    "    traduccion_google = traducir_resena(rese√±a_ejemplo, metodo='google')\n",
    "    print(f\"üåç Traducci√≥n (Google): {traduccion_google}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Google Translate no disponible para traducci√≥n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}