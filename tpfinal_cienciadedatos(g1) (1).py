# -*- coding: utf-8 -*-
"""TPFinal-CienciaDeDatos(G1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F20zapW9ubnjZxdPLk_FzKDQF3JEU7MN
"""
"""Codigo de Die"""

#importaciones
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from transformers import DataCollatorForLanguageModeling, pipeline
import torch

# Leer dataset
# Tomar una muestra balanceada de hasta 600 registros por rating (1-5)
df = pd.read_csv('Amazon_Unlocked_Mobile.csv')
df = df.dropna(subset=['Reviews', 'Rating'])  # eliminamos reseñas y ratings vacíos
df = df[['Product Name', 'Brand Name', 'Price', 'Rating', 'Reviews']]

# Convertir rating a entero (por si acaso)
df['Rating'] = df['Rating'].astype(int)

# Balancear el dataset: igual cantidad de reseñas por rating
max_per_rating = 600
balanced_df = pd.concat([
    df[df['Rating'] == rating].sample(n=min(max_per_rating, len(df[df['Rating'] == rating])), random_state=42)
    for rating in range(1, 6)
])

# Creamos un prompt claro y estructurado
balanced_df['text'] = balanced_df.apply(
    lambda row: f"[INICIO]\nProducto: {row['Product Name'] or 'N/A'}\nMarca: {row['Brand Name'] or 'N/A'}\nPrecio: {row['Price']}\nPuntuación: {row['Rating']} estrellas\n[RESEÑA]\n{row['Reviews']}\n[FIN]",
    axis=1
)

balanced_df['text'] = balanced_df['text'].astype(str)

print('Ejemplo de texto antes de tokenizar:')
print(balanced_df['text'].iloc[0])
print('Tipo:', type(balanced_df['text'].iloc[0]))

dataset = Dataset.from_pandas(balanced_df[['text']])

# 🔠 Tokenizamos
model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    # Reducimos la longitud máxima a 64 tokens
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=64,
    )

print('Ejemplo de tokenización:')
print(tokenize_function({"text": [balanced_df['text'].iloc[0]]}))

tokenized_datasets = dataset.map(tokenize_function, batched=True)

tokenized_datasets = tokenized_datasets.remove_columns(['text'])

# 🔧 Cargamos modelo y configuramos entrenamiento
model = AutoModelForCausalLM.from_pretrained(model_name)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,  # Aumentamos a 3 épocas
    per_device_train_batch_size=8,
    save_steps=500,
    save_total_limit=1,
    logging_steps=100,
    prediction_loss_only=True,
    remove_unused_columns=False
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()

# Guardar modelo y tokenizer de forma compatible con HuggingFace
model.save_pretrained('./results')
tokenizer.save_pretrained('./results')
